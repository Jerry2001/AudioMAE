{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05ce49bf-4178-4168-8697-6f02f69db7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import sys\n",
    "import os\n",
    "import requests\n",
    "import torchaudio\n",
    "from torchaudio.compliance import kaldi\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import importlib\n",
    "import models_mae\n",
    "import librosa\n",
    "import librosa.display\n",
    "from dataset import AudiosetDataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d10d19df-009e-45d9-bafe-aa287aa8d04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MELBINS=128\n",
    "TARGET_LEN=1024\n",
    "def prepare_model(chkpt_dir, arch='mae_vit_base_patch16'):\n",
    "    # build model\n",
    "    model = getattr(models_mae, arch)(in_chans=1, audio_exp=True,img_size=(1024,128),decoder_mode=0)\n",
    "    # load model\n",
    "    checkpoint = torch.load(chkpt_dir, map_location='cpu')\n",
    "    msg = model.load_state_dict(checkpoint['model'], strict=False)\n",
    "    print(msg)\n",
    "    return model\n",
    "def prepare_model1(chkpt_dir, arch='mae_vit_base_patch16'):\n",
    "    # build model\n",
    "    model = getattr(models_mae, arch)(in_chans=1, audio_exp=True,img_size=(1024,128),decoder_mode=1,decoder_depth=16)\n",
    "    # load model\n",
    "    checkpoint = torch.load(chkpt_dir, map_location='cpu')\n",
    "    msg = model.load_state_dict(checkpoint['model'], strict=False)\n",
    "    print(msg)\n",
    "    return model\n",
    "def wav2fbank(filename):\n",
    "\n",
    "    waveform, sr = torchaudio.load(filename)\n",
    "    waveform = waveform - waveform.mean()\n",
    "\n",
    "    # 498 128\n",
    "    fbank = kaldi.fbank(waveform, htk_compat=True, sample_frequency=sr, use_energy=False, \n",
    "                        window_type='hanning', num_mel_bins=MELBINS, dither=0.0, frame_shift=10)\n",
    "    # AudioSet: 1024 (16K sr)\n",
    "    n_frames = fbank.shape[0]\n",
    "    p = TARGET_LEN - n_frames\n",
    "    # cut and pad\n",
    "    if p > 0:\n",
    "        m = torch.nn.ZeroPad2d((0, 0, 0, p))\n",
    "        fbank = m(fbank)\n",
    "    elif p < 0:\n",
    "        fbank = fbank[0:TARGET_LEN, :]\n",
    "    return fbank\n",
    "def norm_fbank(fbank):\n",
    "    norm_mean= -4.2677393\n",
    "    norm_std= 4.5689974\n",
    "    fbank = (fbank - norm_mean) / (norm_std * 2)\n",
    "    return fbank\n",
    "def display_fbank(bank, minmin=None, maxmax=None):\n",
    "    #print(bank.shape, bank.min(), bank.max())\n",
    "    #plt.figure(figsize=(18, 6))\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    plt.imshow(20*bank.T.numpy(), origin='lower', interpolation='nearest', vmax=maxmax, vmin=minmin,  aspect='auto')\n",
    "    #plt.colorbar()\n",
    "    #S_db = librosa.amplitude_to_db(np.abs(bank.T.numpy()),ref=np.max)\n",
    "    #S_db = bank.T.numpy()\n",
    "    #plt.figure()\n",
    "    #librosa.display.specshow(10*bank.T.numpy())\n",
    "    #plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b5a5a9-abeb-4532-ae99-443966d3b783",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(models_mae)\n",
    "chkpt_dir = '../model/audiomae-finetuned/finetuned.pth' \n",
    "\n",
    "model = prepare_model(chkpt_dir, 'mae_vit_base_patch16')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39d85726-3115-48eb-83d7-81c5be68226a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_file = \"../dataset/fsd50k/FSD50K.dev_audio/138309.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1009359-4799-4137-8cf5-1049069505e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wav2fbank' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fbank \u001b[38;5;241m=\u001b[39m \u001b[43mwav2fbank\u001b[49m(wav_file)\n\u001b[1;32m      2\u001b[0m fbank \u001b[38;5;241m=\u001b[39m norm_fbank(fbank)\n\u001b[1;32m      3\u001b[0m display_fbank(fbank)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wav2fbank' is not defined"
     ]
    }
   ],
   "source": [
    "fbank = wav2fbank(wav_file)\n",
    "fbank = norm_fbank(fbank)\n",
    "display_fbank(fbank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2a107db-4173-4185-be98-99fc7ad6a74c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 128])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbank.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4eedcc82-5feb-4424-b282-29aae3ddaa8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 128])\n",
      "torch.Size([1, 1, 1024, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_583286/4133286013.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(fbank)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(fbank)\n",
    "print(x.shape)\n",
    "# make it a batch-like\n",
    "x = x.unsqueeze(dim=0)\n",
    "x = x.unsqueeze(dim=0)\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61e6e697-a238-4e01-b8fa-ee70d07ef918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 513, 768])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    embeddings = model(x, return_embeddings=True, mask_ratio=0)\n",
    "\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4eb2130-7245-45d2-9bee-d9a8932edb43",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mAudiosetDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_conf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_mel_bins\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Preprocess audio \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m/data/scratch/ngop/AudioMAE/dataset.py:136\u001b[0m, in \u001b[0;36mAudiosetDataset.__init__\u001b[0;34m(self, dataset_json_file, audio_conf, label_csv, use_fbank, fbank_dir, roll_mag_aug, load_video, mode)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03mDataset that manages audio recordings\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m:param audio_conf: Dictionary containing the audio loading and preprocessing settings\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03m:param dataset_json_file\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatapath \u001b[38;5;241m=\u001b[39m dataset_json_file\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset_json_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[1;32m    137\u001b[0m     data_json \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(fp)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_fbank \u001b[38;5;241m=\u001b[39m use_fbank\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "dataset = AudiosetDataset(None, audio_conf={'num_mel_bins':128}) \n",
    "\n",
    "# Preprocess audio \n",
    "with torch.no_grad():\n",
    "  spec = dataset._wav2fbank(waveform)\n",
    "  spec = (spec - dataset.norm_mean) / dataset.norm_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eb3667-df4d-4d7f-af37-347aa6fd8aef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audiomae",
   "language": "python",
   "name": "audiomae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
